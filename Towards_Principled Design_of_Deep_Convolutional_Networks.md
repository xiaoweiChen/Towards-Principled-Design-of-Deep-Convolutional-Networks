# 深度卷积网络的原理设计：一个简单的SimpNet

[原文链接](https://arxiv.org/pdf/1802.06205.pdf)

作者：

 * Seyyed Hossein Hasanpour 
* Mohammad Rouhani
* Mohsen Fayyaz
* Mohammad Sabokrou 
* Ehsan Adeli

译者：

* 陈晓伟

## 摘要

近年来，卷积神经网络(CNN)获得非常重大的成功，其代表就有VGGNet、ResNet和DenseNet等等，不过其参数达到了上亿个(几亿到几十亿之多)，这就需要更为关注计算网络所需要的计算资源，以及网络所占用的内存开销。由于这种现实的问题，限制了其在训练和优化的应用。这时，轻量级架构(比如：SqueezeNet)的提出，志在解决以上的问题。不过，因为要在计算资源和高效运行之间进行权衡，所以会产生精度不高的问题。网络低效的问题大部分源自于一种点对点的设计。本文中，在讨论过程中会为构建高效的架构提出几个关键的设计原则，并且会阐述在设计过程中对于不同方面的考虑。此外，我们会介绍一个新层——*SAF-pooling*，在加强网络的归一化能力的同时，通过选择最好的特征保证网络的简单性。根据这些原则，我们提出一个简单的架构，称为*SimpNet*。这个架构仅根据这原则设计，其在计算/存储效率和准确性之间有很好的折衷。*SimpNet*在一些知名性能测试方面的表现，要优于那些更深和更复杂的架构，比如VGGNet、ResNet和WideResidualNet等，其在参数和操作数量上要比这些网络少2到25倍。同时，我们在一些测试集上获得了很不错的结果(在模型精度和参数平衡方面)，比如CIFAR10，CIFAR100，MNIST和SVHN。*SimpNet*的实现可以在这里看到：https://github.com/Coderx7/SimpNet

## 索引词

深度学习，卷积神经网络，简单网络，分类，效率

### 1. 介绍原因

自从神经网络再次复兴以来，深度学习的方法在不同应用领域中都获得了巨大的成功，其中包括语义分割、分类、物体识别、图像标注和自然语言处理[1]。卷积神经网络对于表征学习来说是一大利器，其能在给定的数据中发现复杂的结构，并且以分层的方式对这种数据进行表示[2-4]。因此，在神经网络结构中，很少有人工设置的参数。最近，神经网络的结构发展的共同之处在于其深度和复杂度在不断的增加，这样做使给定的任务能获得更好的准确性。2015年，大规模视觉识别竞赛(ILSVRC)[5]的冠军是ImageNet，其成功之处在于使用了一个非常深的结构，具有152层[2]。当年的亚军也有非常深的结构[3]。这种让网络更深的趋势，一直沿袭至今[6-9]。

随着网络结构的加深，为了提高其辨别能力，计算资源和内存开销需要的越来越多，这也使得很难将这些网络结构用于实际应用，或者对其结构进行扩展。我们对现存的学习算法进行改进，比如：不同的初始化算法[10-13]、归一化和正则化[14-18]、非线性[11, 19-21]和数据增强[4, 14, 22–24]，当在一个性能良好的结构中使用这些方法，那么会受益匪浅。不过，这些方法中可能有些会消耗更多的计算资源，或增加内存开销[6, 15, 16]。因此，特别希望设计出具有低复杂度(少量的层数和参数)的高效网络结构，并且效果与那些更深和更复杂的网络结构一样能得到很好的结果。具有了这样高效的架构，再用刚才提到的方法进行调整，就能获得更好的效果。

设计性能良好的网络结构也有一些关键性的原则，每个原则所对应的目标有所不同，将这些原则结合起来，则能很好的增强网络结构的各个方面。遵循原则性方法，而非是一种点对点的方法。原则性方法具有允许粒度调优和自动化结构设计的优点。换言之，已经通过验证的设计原则可以用来实现一个定制化的架构，或将这些原则有策略的应用在网络架构的不同方面。另外，这种有策略的设计方式能有效的展示哪些方面对当前网络结构更具有影响力，并且可以更好的修正当前存在的问题，而不会对主要问题有所偏离，或是去针对影响力较小的方面。这种影响的展示就是可以使用相关研究，使用更加优化的算法来创建一个结构。因具有更好的识别感知，相关领域的只是可以转移到算法中去。比如，Google的AutoML就旨在自动化的设计机器学习模型[25]。

本文中，我们会介绍一些设计原则，创建一个路线图用于建立符合预期目标的网络结构。我们会对结构的不同方面进行详细的描述，例如：不同的池化操作、卷积核大小和深度等等，为架构中的底层组件提供良好的洞察力。根据我们定义的原则，我们提出了一种的新操作，名为SAF-pooling，其能提升网络的辨识能力。这层中，我们强制网络通过对最高激活值进行池化，从而学习到更加强健的特征。这里的“最高激活值”指的是，与特定类相关的强特征，并在之后会随机的对这些特征进行关闭。在这个过程中，我们模拟了由于遮挡、视点变化、光照变化等原因造成的特征不全的情况，因此网络必须通过找到新的特征检测器来适应这些情况。

考虑这些原则的原因，是因为我们要构建一个新的SimpNet架构，这个架构需要时简单并高效的。为了表明这些原则的高效性，我们会在4个主流的性能数据集(CIFAR10/100、SVHN和MNIST)上进行测试，结果显示我们的架构要优于那些更深、更复杂的网络结构，并且我们的架构减少了2到25倍的参数数量。除此之外，我们也对每个原则的进行了大量的测试，就是为了证明其必要性。简单且高效的架构(比如SimNet)会对这些原则尤为重视。这些原则为很多场景提供了高效和理想的模型架构，特别是对移动端设备(比如无人机、手机)，嵌入式系统(物联网应用)，在云边上的人工智能(AI)和深度学习(DL)应用(比如：从云中获取AI和DL应用到端点设备)。这种网络架构在未来可以使用[26,27]中提到的压缩方法进行压缩，这样对存储器和处理器的要求会有大幅度的下降。我们想要避免设计的混乱，所以采用简单和基础的方法进行设计。这样我们的注意力就不会被其他的因素所干扰，集中精力关注这些原则，确定其对性能的影响。如果不这样，工作量就会变得很大，需要面对许多来自不同方面的挑战，从而导致工作周期拉的很长，并且有可能没法去确定哪些才是真正重要的原则。从而导致远离初心，偏离主题。因此，解决方法就是从现有的文献中找寻相关的方法，并对这些方法分别进行研究。以这种方式，在模型表现的比较理想的情况下，放松对模型的约束(受一些文献中的方法的启发)，这样不需要太大的功夫就能进一步的对性能进行提升。当然，性能的提升与框架设计的水平直接相关。拙劣的设计因为其先天的缺陷，从而无法利用这些原则性的优势达到加速的效果。

本文剩余内容的组织方式：第2节，会对相关工作进行介绍；第3节，会展示我们设计的原则；第4节，展示根据我们的原则设计的SImpNet框架；第5节，展示4大数据集(CIFAR10/100、SVHN和MNIST)的实验结果，网络结构的细节和对于每个数据集预训练的不同之处；第6节，对当前工作的总结，以及对未来发展的展望。

### 2. 相关工作

这一节中，我们将分为两小节来回顾一下最新的趋势和关于网络的相关工作。然而，据我们所知，目前几乎没有对通用网络架构设计原则的研究。[8, 28, 29]在我们之前对适合于特定场景原则或策略进行过研究，虽然其中有一些原则可以应用于任何架构，但其大多数的原则还是只适用于某个特定的框架结构，所以不会对其他的场景进行测试或实验。[30]中只是列出了一些以前使用过的技术，而没有更广义的讨论起有效性和可用性。其只是一个有关使用了哪些技术的报告，并没有说明这些技术在不同情况下的有效性。从方向性上说，[28]与我们的工作最为相似。接下来，我们就简要的聊聊这些年网络结构发展的总体趋势。

#### A. 网络复杂性

从神经网络出现以来，人们都在积极地设计更为有效的网络结构[31-33]。因此出现了更加深层、更加复杂的网络结构[2-4, 6, 9, 22, 34-37]。第一群吃螃蟹的Ciresan等人[34]使用GPU训练了一个9层的*多层感知器*(MLP,  multi-layer precenteron)，然后其他的研究人员也接踵而至[2-4, 6, 9, 21, 34-38]。在其不同的研究领域中，有一些实践标准在创建和设计网络架构上起着非常重要的作用。在2012年Krizhevsky等人[22]创建了一个8层版本的LeNet5，称为AlexNet[39]。相较于LeNet5，AlexNet添加了一种新的归一化层称为局部对比归一化(译者：我看到的资料都是添加了"局部响应归一化(LRN))，和使用校正线性单元(ReLU)[21]作为非线性激活函数，用来取代原始的双曲正切方式(比如：Tanh)。当然，还有一个新的正则化层，称为Dropout层[40]。这个网络在ILSVRC 2012[5]测试集上，在当时达到了世界领先水平。在2013年，Lin等人[41]在论文中引入了一种新的概念，其与方式已经建立好的网络发展趋势完全不同。他们提出的这个概念叫做*网中网*(NIN, network-in-network)，使用一个1x1的滤波器在卷积神经网络中构建微型神经网络，并用全局池化作为结构化正则来代替全连接层，显式的将特征图转换成置信图，并在CIFAR10数据集上达到了当时领先世界的水平。在2014年，Simonyan等人设计的VGGNet[4]引用了几种网络结构，其深度从11层增加到19层，并且表明网络的层级越深，效果越好。他们也使用了3x3卷积层，并说明使用小尺寸的滤波器对非线性有更好的拟合性，同时能获得更好的精度。同年中，受NIN结构的启发产生了一种新的网络结构GoogleNet[3]，56个卷积层构成了一个具有22个模块化层级的网络。模块是由几个1x1，3x3和5x5卷积层构成，这种结构称为inception模块。与以往的网络结构相比，这种结构极大的减少了模型参数的数量，并且在ILSVRC测试中达到了当时的最高水准。在新的Inception版本中，5x5的卷积层被替换成两个连续的3x3卷积层。网络中使用批量归一化[16]可以减少网络内部协变量的偏移；这种方法在训练深层网络结构是十分重要的，并在ImageNet的挑战中刷新了当时的最好成绩。

在2015年，He等人[11]使用VGGNet19[4]再次刷新了ILSVRC的成绩，其使用参数化ReLU层(PReLU)来替代ReLU，从而改进了模型的拟合性。其还引入了一种新的初始化方法，从而能用来增强新非线性激活层的性能。网络层间的连通性在当时是一个全新的概念，其目的就是增强网络中的梯度传递和信息流。相关原理介绍分别在[2]和[37]中。在[2]中，会对残差块进行介绍，说明那些曾适合进行残差映射。在前人的研究基础上，他们成功的训练出从152层到1000层的极深网络结构，并再次刷新了ILSVRC的成绩。同样，[37]中提出了一个递归神经网络(即使用自适应门单元调节网络中的信息流)，其灵感来自于长短期记忆(LSTM)。这种递归神经网络也很成功的训练出从100层到1000层的网络。

在2016年，Szegedy等人[7]将残差链接与他们的Inception-V3结构相结合，具有更好的效果。他们也给出了相关经验证明，使用残差链接对Inception进行训练，可以显著的加速训练速度，并且说明残差版本的Inception网络的性能与原始版本的Inception网络差不多。随着网络这样的发展， ILSVRC 2012分类任务[5]在单帧识别的性能上，有了巨大的提升。Zagoria等人[9]在宽残差网络(WRN)的的残差网络[2]上进行了详细的实验，与以往窄而深的网络不同，其扩展了网络的宽度，而非深度(即降低深度)。这种新的网络结构，并不会有减少特征重用，以及训练时间慢的问题。Huang等人[6]引入了一种新的层间链接方式，称为DenseBlock，其中每层以前馈的方式直接连接到其他的层。这种连通模式减轻了梯度消失的问题，并增强了特征的传播。虽然跨层链接有所增加，但是其鼓励使用特征复用，从而极大的减少了模型参数的数量。这种模型的泛化能力出奇的好，并刷新了好几个测试集的性能成绩，比如CIFAR 10/100和SVHN。

#### B. 轻量级架构

除了让网络更复杂的研究方向之外，一些研究者还对其相反的方向进行研究。Springenberg等人[42]研究了简单结构的可行性。它们打算设计一个简化的架构，不一定要更加的浅，但性能一定要比复杂的网络好。它们建议使用带跨距的卷积来替代池化层，并且证明结构中并不需要池化，并且这样能获得更好的性能。它们对不同版本的结构进行测试，并且其中一个17层的版本(进行了数据扩充)，与当时CIFAR 10的最好城成绩十分接近。在2016年，Iandola等人[29]提出了SqueezeNet架构，这是一种轻量级的CNN架构，其在只有AlexNet的50分之一参数的情况下，在ImageNet挑战中达到了与AlexNet相同的精度。他们使用了一种已存在的技术bottleneck，并且认为空间相关性并不是那么重要，所以可以使用1x1的滤波器代替3x3的滤波器。另外，[43]提出了一个简单的13层结构，为了避免结构过深，参数量过于庞大，其网络只是用3x3的卷积层和2x2的池化层构成。其比那些深度网络少2到25倍的参数量，并在CIFAR 10上的成绩要比更深和更复杂的ResNet更好，并且在没有进行数据扩充的情况下载CIFAR 10 上刷新了当时的最好成绩。其在其他数据集上的成绩也是非常具有竞争力。

在2017年，Howard等人[44]提出了一个有28层结构的MobileNet，其基于一种流线型结构，使用深度可分离卷积来构建轻量级的深度神经网络。他们在ImageNet上进行了测试，实现了VGG16级别的精确度，模型减少了32倍，计算量减少了27倍。几个月后，Zhang等人[45]提出了一个名为ShuffleNet的架构，其专为计算能力有限的移动设备设计(比如10-150MFLOPs的设备)。借鉴*深度可分离卷积*(depth-wise separable convolution)的思想，提出了两种操作，*点级卷积组*(point-wise group convolution)和*通道混洗*(channel shuffle)，这两种操作在保持精度的同时可以大大降低计算量。

本文中，我们会介绍一些可用于设计高效的网络架构的基本原则。使用这些原则，我们设计了一个简单的13层卷积网络，在一些竞赛的性能数据集上进行测试，并取得了非常好的结果和非常好的参数方案。该网络因更少的参数和运算量，几乎优于目前所有更深、更复杂的网络结构。相比于SqueezeNet和FitNet的架构(具有比我们结构更少的参数，但是比我们的结构更深)，我们的网络结构要远远优于这两种结构。这就展示了之后所要提及的原则在设计新网络是的有效性和必要性。对于简单的架构可以根据后文所提到的技术原则和改进意见，对网络结构进行增强。

### 3. 设计原则

设计一个优秀的架构需要对性能和复杂度进行认真地权衡。实际中要处理好这两方面，需要考虑很多的因素。之前的一些研究中，因只基于其特定的应用，所以会忽略一些因素。这里，我们就要来研究一下这些因素，并且对这些因素进行明确的归类和分项，并在设计一个网络的时候，将其作为某个需要考虑的原则。然后，基于这些原则，我们提出了一个网络结构，并说明我们在获得更好的结果的同时，也减少了对计算资源的需求。

#### A. 从最小分配逐渐扩展

在设计一个网络时，其给人的第一个印象就是要设计一个非常深的网络结构。在学术界中，这是一个被广泛接受的想法——更深的网络会有更好的表现。这种现象形成的原因是近年来更深层次网络的成功[2-4, 46-51]。Romero等人[52]还展示了更深和更窄的结构会有更好的表现。通过增加网络层，我们基本上可以为网络提供更多的能力，用来学习不同的知识，以及不同知识间的关系。此外，研究表明前期网络层学习到的是比较低级别的特征，而较深的层学习到的是更加抽象、更加高级的特征，属于某个特定领域的概念。因此，更深层次的特征会产生更好的结果[22, 53]。此外，使用更深的网络结构对复杂的数据集拟合度会更高，而较浅的网络结构则与简单的数据集拟合的比较好[2-4, 22, 48]。虽然深层结构确实比浅层架构有更好的准确性，但是当深度到达一定程度时，它们的性能就会开始下降[28]，并且深层结构的性能要低于浅层结构，这说明浅层架构可能才是更好的选择。因此，有人尝试对宽而浅的网络进行测试，以表明其能力也可以和深而窄的网络一样[9, 43]。为了能有更好的性能，网络的深度必须首先考虑。基于之前的论断，似乎要获得比较好的结果，网络必须要有一定的深度。然而，深度、宽度和参数之间的比例是未知的，并且不同的比例关系所构成的网络的会产生完全不同的结果。基于上述讨论，为了能够很好的利用网络深度、内存和参数，最好的设计方式就是以循序渐进的方式进行，即建议从小而浅的网络开始，逐步深入，然后扩大网络，而不是创建一个很深，且有随机数目的神经元网络。这有助于方式处理单元过度分配的问题，这会造成一些必要的开销，并导致网络过拟合。此外，采用渐进策略有助于对网络中熵的管理。网络保留的参数越多，其收敛速度也就越快，达到的精度也会越高，但也更容易过拟合。当具有较少参数的模型能提供与复杂模型相当的结果或性能，就表明网络已经学习到了用于完成该任务的特征。换句话说，通过对网络熵添加更多的约束，网络被迫需要寻找和学习更加健壮的特征。网络基于更重要、更具有辨别力和更少噪声去做判断的能力，决定了其泛化的能力。通过为网络分配足够的容量，更浅和更宽的网络要比更深和更窄的网络执行的更好，我们的实验结果能证明这一点，并且[8, 9, 28]也指出了这一点。结果还表明，将现有的残差结构进行拓宽，将会比将其加深的性能更好[9]。因此，有一定深度的更宽和更浅的网络相较于更深的网络来说，是更好的选择。此外，由于GPU在并行计算中对大型的张量处理效率非常高，因此增加网络层的宽度，要比单独数千个小卷积核的计算更加高效[9]。

如果在此阶段导致网络的深度过深，则会导致一些问题(例如，梯度流弱化、网络退化[2]和增加计算开销)，而在许多应用中通常并不需要十分深的网络层。建议以金字塔的形状对网络进行扩充，这就意味着特征图数量的增加，特征图的空间分辨率逐渐降低，从而保持特征的表现力。通过增加特征的丰富度(特征图的数量)[39]来补偿空间分辨率的降低，从而保证对输入集合最大程度的不变性。有趣的是，在相同的参数数量下，更深版本可能无法达到浅层网络的精度，这看上去与我们之前讨论的有些矛盾。因为浅网络结构能更方便对层处理能力的分配，所以性能方面会有较好的改进。网络架构中的每一层都需要分配特定数量的处理单元，从而能够正确的执行相应的任务。因此，在参数数量相同的情况下，浅层结构相较深层结构参数会更好的分配在层与层之间。显然，使用相同计算资源的硬件，在相对较深的网络架构中，每层中需要处理的单元很少，因此会导致网络性能退化。在这种情况下，我们可以说在层级方面遇到了欠拟合或容量过低的问题。降低处理能力将无法利用网络深度的优势，因此网络无法正常运行。在更深的网络结构中，处理单元分配不均的几率非常高。因此，在处理能力相同的情况下，计算单元在神经元之间的分配上，更深的网络更容易不均。当网络解决越来越深的时候，这个问题会更加凸显。换句话说，随着网络结构深度的加深，很难将神经元分配到所有层上，并且在某时所给定的计算资源将无法让网络正常运行。当更深的网络遇到缺少处理能力的问题时，就可以认为这个网络结构还不够简单，其需要必要的功能保证数据被正确的表示，这种现象我们称为**处理级枯竭**。这会导致更深的网络架构不如比较浅的网络架构。这个问题是深层网络与浅层网络性能层面比较重要的比较。因此，随着计算能力的增加，浅层网络的性能要低于更深层的网络结构，并且随着计算能力的逐步增加，这个趋势会变得越来越明显。这是因为浅层网络架构在计算量上已经饱和，会让计算力过剩，我们将这种现象称为**处理级饱和**现象，其中过剩的处理能力没有被充分的利用，所以网络结构的性能达到了饱和的状态。在计算能力充足的情况下，更深的网络架构能够更多的利用计算资源，开发出更有趣的功能，从而处理极枯竭的现象也会有进一步的改善。随着参数数量的增加，这种差异更加的明显。相应的，因为更深的网络架构在参数数量上要比更浅的网络架构更多。需要注意的是，放我们考虑增加计算能力时(适合更深的网络架构，这样所有的层都有足够的算力来执行相应的任务)，相同数量的参数会在更浅的网络结构中显得过于饱和，从而导致计算力过剩(反之亦然)。这也说明了为什么逐渐扩展和最小分配能成为关键概念，因为它们不会让你一开始就设计非常深的网络架构，其次阻止分配过多的神经元，从而有效避免处理级枯竭和处理级饱和的问题。

#### B. 同类层组

传统的网络结构设计，可以看作将若干类型的网络层(卷积、池化、归一化等)堆叠在一起。与其将设计过程看作是简单的堆叠过程，不如将网络结构看成由同样层构成的组。由几个同类型的层组，每个层组负责特定的任务(实现一个目标)。这种对称和同质的设计，不仅可以很容易地管理网络中的参数数量，同时也为每个语义提供了更好的信息池，而且还为以分组的方式，为达到更细粒度的微调和参数更新，提供了可能性。自从[22]的方法成功以来，大家都在心照不宣的在使用这项技术，并且这项技术目前已经越来越多的应用在当前主流到的网络架构中，其中就包括[2, 6-9, 54]。当然，以前这种技术只是为了让概念听起来很炫，而非对这种构建块进行充分的利用。因此，这种特征其他方面的优点并未充分利用起来。换句话说，之前大多数的用例都是这种形式，其利用了网络内部的点对点方式。尽管如此，这种方案还是对网络拓扑管理需求和参数管理大有帮助的。

#### C. 保留局部关联性

保持网络中的局部性是非常重要的，尤其是在前期的层中要避免使用1x1的卷积核。卷积神经网络(CNN)成功的基石就依赖于局部相关性的保留[39, 55]。不过，[29]的作者们有着相反的想法，在它们的设计中，更多是用了1x1的卷积(而不是3x3的卷积)，并且也得到了非常好的结果，这样看来在卷积神经网络中空间分辨率并不是那么得重要。我们的实验经验和其他文献[4, 8, 39]中都得到的是相反的结论，所以我们认为得到那样的结论很可能是遵循了某种特定的前提假定，当经过更加完整和彻底的试验后，可能就会得到不同的结果。举个例子，滤波器在网络中的分布并没有一个原则性的准则可以参照，对于特定的网络架构设计也是一样的，所以目前的网络没办法充分利用大一些的卷积核提供的内容信息。其中的一个原因可能是过度的使用了bottleneck技术，对特征表示过度的压缩，从而导致了这样的结果。更进一步的实验证明了我们的想法，仅使用3x3卷积核具有较少参数的更浅层网络，性能要好过与具有其两倍参数的SqueezeNet。基于我们的实验结果，建议前期层不要使用1x1卷积层或全连接层，信息的局部性问题对于网络前期层尤为重要，1x1的卷积核也具有很多功能，比如提升网络非线性和融合特征[41]的能力，以及根据因式分解和类似技巧[8]减少网络参数的特性。不过，从另一方面来说，他们忽略了输入中的局部相关性。由于只考虑通道信息，而不考虑输入中的领域，因此他们曲解了局部性信息。除了前期以外，1x1的卷积层还是优先的选择。如果打算在网络终端层之外的地方使用1x1卷积层，建议使用2x2卷积层代替1x1卷积层。使用2x2的卷积层既减少了参数的数量，同时也保留了邻域的信息。同时还应该注意的是，应该避免使用1x1卷积层对bottleneck策略中的特征表示进行过度的压缩，否则会损坏信息[8]。

虽然[8]中大量的使用聚合和因式分解等技术，将大的卷积核替换成小的卷积核，并能大量减少计算量，不过由于以下一些原因，我们不会使用这种方式。

1. 为了尽可能的保证简洁，需要对基本元素进行评估，并确定关键元素，这些元素在之后的工作中要能合并到其他方案中，这就包括类似Inception的模块。
2. 尝试将当前网络中的卷积核全部使用一系列连续更小的卷积核进行替代(使用两个3x3或四个2x2的卷积替换一个5x5)，这里对于其有效性和增加的网络深度不进行追究的话，对于网络效率的把控将会变得特别困难。
3. 需要了解不同大小的卷积核的有效性，从而准确利用不同大小的卷积核来提升网络的性能。
4. 多路径设计已经使用在Inception等模块上[3, 7, 8]，其将几个具有不同卷积核尺寸或数量的卷积层直接相连，并且目前没有足够的证据表明其工作的独立性[28]。当有内核在旁边作为补充时，这种网络会有很好的效果。因此，它们独立工作的效果需要更多的研究和实验来论证。此外，关于选择这种多路径的网络结构，对于其超参的设计原理也还需要进一步进行研究。目前还有一些困惑：每个分支的起到的作用，并不是那么清楚[28]，因此我们只考虑没有平行卷积层的"单路径"设计(在面临大量选择的情况下)。
5. 像Inception这样的模块，确实减少了参数的数量[8]，但其会带来计算上的开销。因为其复杂性，要考虑的变量有很多，其定制化过程会十分困难。所以，我们选择但路径设计。我们至少使用了这些原则，提升了网络的效果。

最后，需要注意的是，在前期层中具有较少的特征尺寸的情况下，避免在要结束的几个层将特征尺寸缩小的太多，以及分配过多的神经元给它们。网络末端是使用小的特征图尺寸(比如1x1)，会带来几乎可以忽略的信息增益，并且分配过多的神经元会导致内存的浪费。

#### D. 最大化信息利用率

前期网络层中还有一点非常重要，避免过快的进行下采样或池化。[8, 28, 29]也给出过类似的建议。因为需要更多的信息来提升网络的鉴别能力。所以也需要通过更大的数据集(收集更多的数据，或者对现有数据进行有效的扩充)，或者使用更大的特征图和更好的信息池对信息进行充分的利用。技术方面，比如Inception[3, 7, 8, 16]，残差网络的层间连通性[2]，密集连接[6]和池化融合[56]，这些比较复杂的方式都能提供更好的信息池。不过，至少在目前的网络基本形式中，架构可以在没有这些技术的帮助下，实现更好的信息池。如果没有特别大的数据集可用，那么必须有效的利用当前训练样本。在网络架构前期，较大的特征图与较小的特征图相比，能为网络提供更有价值的信息。在相同深度和参数数量的前提下，如果网络能利用更大的特征图，则能获得较高的精确度[4, 8, 28]。因此，与其通过增加网络的深度和参数来增加网络的复杂性，还不如使用较大的输入维度或避免快速的进行下采样，从而获取更好的结果。这是一种检查网络网络复杂程度的技术，并且能提高准确性。类似的观点在[3, 4, 8]中也有提及。虽然尽管参数很少，但使用较大的特征图来表示信息池可能会导致较大的内存消耗。DenseNet就是一个例子，其模型(DenseNet-BC, L = 100, k = 12, 参数只有0.8M)需要占用超过8G的GPU内存(视频内存)。这种内存消耗部分原因可能是实现代码写的比较渣，但即便是进行了充分的优化之后，其内存的占用还是非常的大。其他案例可以参考[2]和[9]的内容，也是使用了很多特别大的特征图。这就说明，这些网络架构的成功，很大程度上要归功于信息池，因为在信息池中的特征图数量是在增加的，但是尺寸并未减小(没有进行下采样)。除了使用平移不变性[57]对数据进行增强外，(最大)池化在降低输出对位移和失真[39]的敏感度方面起着重要的作用。其对转换不变性的实现也非常有帮助[28, 58]。Scherner等人[58]指出相较于二次采样操作，最大池化操作要优于捕获图像数据中不变性的技术。因此，在网络结构中安排合理数量的池化层是非常重要的。使用最大池化会对准确定位有所影响，因此池化层的过量使用会对一些应用场景产生负面的影响，比如语义分割和物体检测。合理的使用池化技能获得转换不变性，又能减少内存和计算资源的占用。还需要注意的是，池化操作本质上并不等同于简单的下采样。理论上，在池化操作完成后，卷积层能学的更好。网络可以从池化操作后学得正确的东西，这已经被证明过很多次了[15, 56, 59, 60]，不过具有较大步长的卷积层[42]还远没做到这点。因此，比如跨距卷积之类的技术不会与池化操作有同样的效果，因为一个简单卷积层需要经过带校正的线性激活，其本身无法实现p范数计算[42]。此外，[42]还指出当池化仅仅是用作下采样处理的话，那么可以使用带有跨距的卷积对其进行替代。然而，我们结合实验结果进行了更进一步的思考，通过合并池化(稍后解释)并不能提升网络结构的性能。我们的测试例中更加明显，在具有相同数量参数的情况下，具有简单的最大池化层的网络结构，性能要优于具有跨度步长的卷积的网络结构。其中依据在我们实验结果章节会进行详实的展示。

#### E. 最大化性能利用率

结合实现细节和当前的经过优化的底层库，可以非常的简单的设计出性能优良、效率超高的网络架构。例如，NVIDIA的cudnn 5.x或更高版本，使用该库实现3x3卷积，除了已知的一些优点[4]之外，其还能给予性能上的显著提升。就如同图1和表1展示的那样，与其v4版本相比，v5的速度要快2.7x；在更新版本的cudnn v7.x版本中，性能提升是非常明显的。

![](images/1.png)



*图1：对不同网络架构的加速进行对比。在3x3的卷积核使用cuDNN v5.x后，图中的训练速度提升了2.7x。*



|            | k80+cuDNN 6 | P100+cuDNN 6 | V100+cuDNN 7 |
| :--------: | :---------: | :----------: | :----------: |
| 2.5x + CNN |     100     |     200      |     600      |
| 3x + LSTM  |     1x      |      2x      |      6x      |

*表格1：使用cuDNN v7.x能很大程度的改善性能*

当设计到工业生产时，充分利用算力来提升性能是十分重要的。不过，在使用5x5和7x7这样的大卷积核计算时，计算开销通常会非常的高昂。例如，在一个具有m个卷积核的网络上有n个5x5的卷积核，其计算成本就要比相同数目的3x3卷积核高2.78倍(25/9)。当然，5x5的卷积核可以在前期网络层中，获取激活单元之间的信息相关性，因此减小卷积核的尺寸可能会耗费更大的成本[8]。除了性能之外，较大卷积核上的每个参数，无法像3x3卷积核上的参数那么高效。我们也许可以这样理解，由于较大的卷积核对输入数据较大的领域空间进行捕获，因此可以在一定程度上对噪声进行抑制，从而能够更好的捕获输入数据的特征。不过，在实际中，大卷积核所占用的计算资源，是其无法成为一个理想的选择。而且，较大的卷积核可以分解为多个小卷积核[8]，因此使用大卷积核使得每个参数的效率下降，从而造成不必要的计算负担。使用小卷积核代替大卷积核，在曾在[7, 8]中进行过研究和探讨。另一方面，小卷积核(包括3x3的卷积核在内)都不会去捕获数据的局部相关性。一组级联的3x3卷积核可以代替任意一个比较大的卷积核，且能达到相同的感受野。此外，入职前所说，这样做也能带来性能上的提升。

#### F. 均衡方案

通常，我们会建议在网络中等比例的分配神经元个数，因为不均匀的分配会导致某些层中的信息短缺，从而造成操作无效。例如，如果前期层没有足够的神经元来容纳低级特征，或者隐藏层没有足够的空间容纳中级特征，那么输出层则没有足够的信息进行抽象。这适用于网络的语义层，也就是如果输出层没有足够的容量，那么网络将不能为精准判断提供更高级别的语义抽象了。因此，为了增加网络的容量(也就是神经元的数量)，最好就是将神经元散布在整个网络中，而不是仅仅添加在一个或几个特定的层中[8]。在深度学习架构中神经元分布不均，就是导致网络结构退化的原因之一。随着网络结构的加深，要在整个网络中适当的分配处理能力变得越来越困难，这个原因也会导致一些较深层的网络架构表现不佳。这实际上就是处理级饱和和枯竭问题的原因。我们将在实验结果章节对此进行详细的描述。使用均匀分配的方案，可以让网络中的所有语义层的容量进行增加，并相应的能为性能做出贡献；反之，若仅为某些特定的层增加容量，增加的这些容量可能会被浪费。

####  G. 快速成型技术

对网络架构进行调整前，使用不同的学习策略对网络架构进行测试是非常有益的。大多数时候，需要改变的并不是网络架构，而是优化策略。选择了不适合的优化策略会导致网络结构的效率低下，从而浪费网络资源。一些很简单的超参，比如学习速率和正则化方法，如果没有进行正确的调整正，通常会对网络结构产生不利的影响。因此，建议首先使用自动优化策略对网络结构进行快速的测试，当网络架构完成时，仔细调整优化策略可以使网络的性能最大化。我们在对其中的某个新特性进行测试或修改时，需要保证其他的设置是不变的。例如，当测试5x5和3x3的卷积时，需要保证网络整体的熵保持不变。在很多实验中，这条规则经常被忽略，这将导致测试结果的不准确，或是得到更加糟糕的推断结果。

#### H. 利用Dropout



#### I. 简单的自适应特征组合池



#### J. 最终阶段的规则



### 4. SimpNet



### 5. 实验结果



### 6. 总结



### 鸣谢



# 补充部分



## S1.  深入讨论



## S2.  网络结构实验

