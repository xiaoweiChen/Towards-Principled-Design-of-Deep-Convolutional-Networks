# 深度卷积网络的原理设计：一个简单的SimpNet

[原文链接](https://arxiv.org/pdf/1802.06205.pdf)

作者：

 * Seyyed Hossein Hasanpour 
* Mohammad Rouhani
* Mohsen Fayyaz
* Mohammad Sabokrou 
* Ehsan Adeli

译者：

* 陈晓伟

## 摘要

近年来，卷积神经网络(CNN)获得非常重大的成功，其代表就有VGGNet、ResNet和DenseNet等等，不过其参数达到了上亿个(几亿到几十亿之多)，这就需要更为关注计算网络所需要的计算资源，以及网络所占用的内存开销。由于这种现实的问题，限制了其在训练和优化的应用。这时，轻量级架构(比如：SqueezeNet)的提出，志在解决以上的问题。不过，因为要在计算资源和高效运行之间进行权衡，所以会产生精度不高的问题。网络低效的问题大部分源自于一种点对点的设计。本文中，在讨论过程中会为构建高效的架构提出几个关键的设计原则，并且会阐述在设计过程中对于不同方面的考虑。此外，我们会介绍一个新层——*SAF-pooling*，在加强网络的归一化能力的同时，通过选择最好的特征保证网络的简单性。根据这些原则，我们提出一个简单的架构，称为*SimpNet*。这个架构仅根据这原则设计，其在计算/存储效率和准确性之间有很好的折衷。*SimpNet*在一些知名性能测试方面的表现，要优于那些更深和更复杂的架构，比如VGGNet、ResNet和WideResidualNet等，其在参数和操作数量上要比这些网络少2到25倍。同时，我们在一些测试集上获得了很不错的结果(在模型精度和参数平衡方面)，比如CIFAR10，CIFAR100，MNIST和SVHN。*SimpNet*的实现可以在这里看到：https://github.com/Coderx7/SimpNet

## 索引词

深度学习，卷积神经网络，简单网络，分类，效率

### 1. 介绍原因

自从神经网络再次复兴以来，深度学习的方法在不同应用领域中都获得了巨大的成功，其中包括语义分割、分类、物体识别、图像标注和自然语言处理[1]。卷积神经网络对于表征学习来说是一大利器，其能在给定的数据中发现复杂的结构，并且以分层的方式对这种数据进行表示[2-4]。因此，在神经网络结构中，很少有人工设置的参数。最近，神经网络的结构发展的共同之处在于其深度和复杂度在不断的增加，这样做使给定的任务能获得更好的准确性。2015年，大规模视觉识别竞赛(ILSVRC)[5]的冠军是ImageNet，其成功之处在于使用了一个非常深的结构，具有152层[2]。当年的亚军也有非常深的结构[3]。这种让网络更深的趋势，一直沿袭至今[6-9]。

随着网络结构的加深，为了提高其辨别能力，计算资源和内存开销需要的越来越多，这也使得很难将这些网络结构用于实际应用，或者对其结构进行扩展。我们对现存的学习算法进行改进，比如：不同的初始化算法[10-13]、归一化和正则化[14-18]、非线性[11, 19-21]和数据增强[4, 14, 22–24]，当在一个性能良好的结构中使用这些方法，那么会受益匪浅。不过，这些方法中可能有些会消耗更多的计算资源，或增加内存开销[6, 15, 16]。因此，特别希望设计出具有低复杂度(少量的层数和参数)的高效网络结构，并且效果与那些更深和更复杂的网络结构一样能得到很好的结果。具有了这样高效的架构，再用刚才提到的方法进行调整，就能获得更好的效果。

设计性能良好的网络结构也有一些关键性的原则，每个原则所对应的目标有所不同，将这些原则结合起来，则能很好的增强网络结构的各个方面。遵循原则性方法，而非是一种点对点的方法。原则性方法具有允许粒度调优和自动化结构设计的优点。换言之，已经通过验证的设计原则可以用来实现一个定制化的架构，或将这些原则有策略的应用在网络架构的不同方面。另外，这种有策略的设计方式能有效的展示哪些方面对当前网络结构更具有影响力，并且可以更好的修正当前存在的问题，而不会对主要问题有所偏离，或是去针对影响力较小的方面。这种影响的展示就是可以使用相关研究，使用更加优化的算法来创建一个结构。因具有更好的识别感知，相关领域的只是可以转移到算法中去。比如，Google的AutoML就旨在自动化的设计机器学习模型[25]。

本文中，我们会介绍一些设计原则，创建一个路线图用于建立符合预期目标的网络结构。我们会对结构的不同方面进行详细的描述，例如：不同的池化操作、卷积核大小和深度等等，为架构中的底层组件提供良好的洞察力。根据我们定义的原则，我们提出了一种的新操作，名为SAF-pooling，其能提升网络的辨识能力。这层中，我们强制网络通过对最高激活值进行池化，从而学习到更加强健的特征。这里的“最高激活值”指的是，与特定类相关的强特征，并在之后会随机的对这些特征进行关闭。在这个过程中，我们模拟了由于遮挡、视点变化、光照变化等原因造成的特征不全的情况，因此网络必须通过找到新的特征检测器来适应这些情况。

考虑这些原则的原因，是因为我们要构建一个新的SimpNet架构，这个架构需要时简单并高效的。为了表明这些原则的高效性，我们会在4个主流的性能数据集(CIFAR10/100、SVHN和MNIST)上进行测试，结果显示我们的架构要优于那些更深、更复杂的网络结构，并且我们的架构减少了2到25倍的参数数量。除此之外，我们也对每个原则的进行了大量的测试，就是为了证明其必要性。简单且高效的架构(比如SimNet)会对这些原则尤为重视。这些原则为很多场景提供了高效和理想的模型架构，特别是对移动端设备(比如无人机、手机)，嵌入式系统(物联网应用)，在云边上的人工智能(AI)和深度学习(DL)应用(比如：从云中获取AI和DL应用到端点设备)。这种网络架构在未来可以使用[26,27]中提到的压缩方法进行压缩，这样对存储器和处理器的要求会有大幅度的下降。我们想要避免设计的混乱，所以采用简单和基础的方法进行设计。这样我们的注意力就不会被其他的因素所干扰，集中精力关注这些原则，确定其对性能的影响。如果不这样，工作量就会变得很大，需要面对许多来自不同方面的挑战，从而导致工作周期拉的很长，并且有可能没法去确定哪些才是真正重要的原则。从而导致远离初心，偏离主题。因此，解决方法就是从现有的文献中找寻相关的方法，并对这些方法分别进行研究。以这种方式，在模型表现的比较理想的情况下，放松对模型的约束(受一些文献中的方法的启发)，这样不需要太大的功夫就能进一步的对性能进行提升。当然，性能的提升与框架设计的水平直接相关。拙劣的设计因为其先天的缺陷，从而无法利用这些原则性的优势达到加速的效果。

本文剩余内容的组织方式：第2节，会对相关工作进行介绍；第3节，会展示我们设计的原则；第4节，展示根据我们的原则设计的SImpNet框架；第5节，展示4大数据集(CIFAR10/100、SVHN和MNIST)的实验结果，网络结构的细节和对于每个数据集预训练的不同之处；第6节，对当前工作的总结，以及对未来发展的展望。

### 2. 相关工作

这一节中，我们将分为两小节来回顾一下最新的趋势和关于网络的相关工作。然而，据我们所知，目前几乎没有对通用网络架构设计原则的研究。[8, 28, 29]在我们之前对适合于特定场景原则或策略进行过研究，虽然其中有一些原则可以应用于任何架构，但其大多数的原则还是只适用于某个特定的框架结构，所以不会对其他的场景进行测试或实验。[30]中只是列出了一些以前使用过的技术，而没有更广义的讨论起有效性和可用性。其只是一个有关使用了哪些技术的报告，并没有说明这些技术在不同情况下的有效性。从方向性上说，[28]与我们的工作最为相似。接下来，我们就简要的聊聊这些年网络结构发展的总体趋势。

#### A. 网络复杂性

从神经网络出现以来，人们都在积极地设计更为有效的网络结构[31-33]。因此出现了更加深层、更加复杂的网络结构[2-4, 6, 9, 22, 34-37]。第一群吃螃蟹的Ciresan等人[34]使用GPU训练了一个9层的*多层感知器*(MLP,  multi-layer precenteron)，然后其他的研究人员也接踵而至[2-4, 6, 9, 21, 34-38]。在其不同的研究领域中，有一些实践标准在创建和设计网络架构上起着非常重要的作用。在2012年Krizhevsky等人[22]创建了一个8层版本的LeNet5，称为AlexNet[39]。相较于LeNet5，AlexNet添加了一种新的归一化层称为局部对比归一化(译者：我看到的资料都是添加了"局部响应归一化(LRN))，和使用校正线性单元(ReLU)[21]作为非线性激活函数，用来取代原始的双曲正切方式(比如：Tanh)。当然，还有一个新的正则化层，称为Dropout层[40]。这个网络在ILSVRC 2012[5]测试集上，在当时达到了世界领先水平。在2013年，Lin等人[41]在论文中引入了一种新的概念，其与方式已经建立好的网络发展趋势完全不同。他们提出的这个概念叫做*网中网*(NIN, network-in-network)，使用一个1x1的滤波器在卷积神经网络中构建微型神经网络，并用全局池化作为结构化正则来代替全连接层，显式的将特征图转换成置信图，并在CIFAR10数据集上达到了当时领先世界的水平。在2014年，Simonyan等人设计的VGGNet[4]引用了几种网络结构，其深度从11层增加到19层，并且表明网络的层级越深，效果越好。他们也使用了3x3卷积层，并说明使用小尺寸的滤波器对非线性有更好的拟合性，同时能获得更好的精度。同年中，受NIN结构的启发产生了一种新的网络结构GoogleNet[3]，56个卷积层构成了一个具有22个模块化层级的网络。模块是由几个1x1，3x3和5x5卷积层构成，这种结构称为inception模块。与以往的网络结构相比，这种结构极大的减少了模型参数的数量，并且在ILSVRC测试中达到了当时的最高水准。在新的Inception版本中，5x5的卷积层被替换成两个连续的3x3卷积层。网络中使用批量归一化[16]可以减少网络内部协变量的偏移；这种方法在训练深层网络结构是十分重要的，并在ImageNet的挑战中刷新了当时的最好成绩。

在2015年，He等人[11]使用VGGNet19[4]再次刷新了ILSVRC的成绩，其使用参数化ReLU层(PReLU)来替代ReLU，从而改进了模型的拟合性。其还引入了一种新的初始化方法，从而能用来增强新非线性激活层的性能。网络层间的连通性在当时是一个全新的概念，其目的就是增强网络中的梯度传递和信息流。相关原理介绍分别在[2]和[37]中。在[2]中，会对残差块进行介绍，说明那些曾适合进行残差映射。在前人的研究基础上，他们成功的训练出从152层到1000层的极深网络结构，并再次刷新了ILSVRC的成绩。同样，[37]中提出了一个递归神经网络(即使用自适应门单元调节网络中的信息流)，其灵感来自于长短期记忆(LSTM)。这种递归神经网络也很成功的训练出从100层到1000层的网络。

在2016年，Szegedy等人[7]将残差链接与他们的Inception-V3结构相结合，具有更好的效果。他们也给出了相关经验证明，使用残差链接对Inception进行训练，可以显著的加速训练速度，并且说明残差版本的Inception网络的性能与原始版本的Inception网络差不多。随着网络这样的发展， ILSVRC 2012分类任务[5]在单帧识别的性能上，有了巨大的提升。Zagoria等人[9]在宽残差网络(WRN)的的残差网络[2]上进行了详细的实验，与以往窄而深的网络不同，其扩展了网络的宽度，而非深度(即降低深度)。这种新的网络结构，并不会有减少特征重用，以及训练时间慢的问题。Huang等人[6]引入了一种新的层间链接方式，称为DenseBlock，其中每层以前馈的方式直接连接到其他的层。这种连通模式减轻了梯度消失的问题，并增强了特征的传播。虽然跨层链接有所增加，但是其鼓励使用特征复用，从而极大的减少了模型参数的数量。这种模型的泛化能力出奇的好，并刷新了好几个测试集的性能成绩，比如CIFAR 10/100和SVHN。

#### B. 轻量级架构

除了让网络更复杂的研究方向之外，一些研究者还对其相反的方向进行研究。Springenberg等人[42]研究了简单结构的可行性。它们打算设计一个简化的架构，不一定要更加的浅，但性能一定要比复杂的网络好。它们建议使用带跨距的卷积来替代池化层，并且证明结构中并不需要池化，并且这样能获得更好的性能。它们对不同版本的结构进行测试，并且其中一个17层的版本(进行了数据扩充)，与当时CIFAR 10的最好城成绩十分接近。在2016年，Iandola等人[29]提出了SqueezeNet架构，这是一种轻量级的CNN架构，其在只有AlexNet的50分之一参数的情况下，在ImageNet挑战中达到了与AlexNet相同的精度。他们使用了一种已存在的技术bottleneck，并且认为空间相关性并不是那么重要，所以可以使用1x1的滤波器代替3x3的滤波器。另外，[43]提出了一个简单的13层结构，为了避免结构过深，参数量过于庞大，其网络只是用3x3的卷积层和2x2的池化层构成。其比那些深度网络少2到25倍的参数量，并在CIFAR 10上的成绩要比更深和更复杂的ResNet更好，并且在没有进行数据扩充的情况下载CIFAR 10 上刷新了当时的最好成绩。其在其他数据集上的成绩也是非常具有竞争力。

在2017年，Howard等人[44]提出了一个有28层结构的MobileNet，其基于一种流线型结构，使用深度可分离卷积来构建轻量级的深度神经网络。他们在ImageNet上进行了测试，实现了VGG16级别的精确度，模型减少了32倍，计算量减少了27倍。几个月后，Zhang等人[45]提出了一个名为ShuffleNet的架构，其专为计算能力有限的移动设备设计(比如10-150MFLOPs的设备)。借鉴*深度可分离卷积*(depth-wise separable convolution)的思想，提出了两种操作，*点级卷积组*(point-wise group convolution)和*通道混洗*(channel shuffle)，这两种操作在保持精度的同时可以大大降低计算量。

本文中，我们会介绍一些可用于设计高效的网络架构的基本原则。使用这些原则，我们设计了一个简单的13层卷积网络，在一些竞赛的性能数据集上进行测试，并取得了非常好的结果和非常好的参数方案。该网络因更少的参数和运算量，几乎优于目前所有更深、更复杂的网络结构。相比于SqueezeNet和FitNet的架构(具有比我们结构更少的参数，但是比我们的结构更深)，我们的网络结构要远远优于这两种结构。这就展示了之后所要提及的原则在设计新网络是的有效性和必要性。对于简单的架构可以根据后文所提到的技术原则和改进意见，对网络结构进行增强。

### 3. 设计原则

设计一个优秀的架构需要对性能和复杂度进行认真地权衡。实际中要处理好这两方面，需要考虑很多的因素。之前的一些研究中，因只基于其特定的应用，所以会忽略一些因素。这里，我们就要来研究一下这些因素，并且对这些因素进行明确的归类和分项，并在设计一个网络的时候，将其作为某个需要考虑的原则。然后，基于这些原则，我们提出了一个网络结构，并说明我们在获得更好的结果的同时，也减少了对计算资源的需求。

#### A. 从最小分配逐渐扩展

在设计一个网络时，其给人的第一个印象就是要设计一个非常深的网络结构。在学术界中，这是一个被广泛接受的想法——更深的网络会有更好的表现。这种现象形成的原因是近年来更深层次网络的成功[2-4, 46-51]。Romero等人[52]还展示了更深和更窄的结构会有更好的表现。通过增加网络层，我们基本上可以为网络提供更多的能力，用来学习不同的知识，以及不同知识间的关系。此外，研究表明前期网络层学习到的是比较低级别的特征，而较深的层学习到的是更加抽象、更加高级的特征，属于某个特定领域的概念。因此，更深层次的特征会产生更好的结果[22, 53]。此外，使用更深的网络结构对复杂的数据集拟合度会更高，而较浅的网络结构则与简单的数据集拟合的比较好[2-4, 22, 48]。虽然深层结构确实比浅层架构有更好的准确性，但是当深度到达一定程度时，它们的性能就会开始下降[28]，并且深层结构的性能要低于浅层结构，这说明浅层架构可能才是更好的选择。因此，有人尝试对宽而浅的网络进行测试，以表明其能力也可以和深而窄的网络一样[9, 43]。为了能有更好的性能，网络的深度必须首先考虑。基于之前的论断，似乎要获得比较好的结果，网络必须要有一定的深度。然而，深度、宽度和参数之间的比例是未知的，并且不同的比例关系所构成的网络的会产生完全不同的结果。基于上述讨论，为了能够很好的利用网络深度、内存和参数，最好的设计方式就是以循序渐进的方式进行，即建议从小而浅的网络开始，逐步深入，然后扩大网络，而不是创建一个很深，且有随机数目的神经元网络。这有助于方式处理单元过度分配的问题，这会造成一些必要的开销，并导致网络过拟合。此外，采用渐进策略有助于对网络中熵的管理。网络保留的参数越多，其收敛速度也就越快，达到的精度也会越高，但也更容易过拟合。当具有较少参数的模型能提供与复杂模型相当的结果或性能，就表明网络已经学习到了用于完成该任务的特征。换句话说，通过对网络熵添加更多的约束，网络被迫需要寻找和学习更加健壮的特征。网络基于更重要、更具有辨别力和更少噪声去做判断的能力，决定了其泛化的能力。通过为网络分配足够的容量，更浅和更宽的网络要比更深和更窄的网络执行的更好，我们的实验结果能证明这一点，并且[8, 9, 28]也指出了这一点。结果还表明，将现有的残差结构进行拓宽，将会比将其加深的性能更好[9]。因此，有一定深度的更宽和更浅的网络相较于更深的网络来说，是更好的选择。此外，由于GPU在并行计算中对大型的张量处理效率非常高，因此增加网络层的宽度，要比单独数千个小卷积核的计算更加高效[9]。

如果在此阶段导致网络的深度过深，则会导致一些问题(例如，梯度流弱化、网络退化[2]和增加计算开销)，而在许多应用中通常并不需要十分深的网络层。建议以金字塔的形状对网络进行扩充，这就意味着特征图数量的增加，特征图的空间分辨率逐渐降低，从而保持特征的表现力。通过增加特征的丰富度(特征图的数量)[39]来补偿空间分辨率的降低，从而保证对输入集合最大程度的不变性。有趣的是，在相同的参数数量下，更深版本可能无法达到浅层网络的精度，这看上去与我们之前讨论的有些矛盾。因为浅网络结构能更方便对层处理能力的分配，所以性能方面会有较好的改进。网络架构中的每一层都需要分配特定数量的处理单元，从而能够正确的执行相应的任务。因此，在参数数量相同的情况下，浅层结构相较深层结构参数会更好的分配在层与层之间。显然，使用相同计算资源的硬件，在相对较深的网络架构中，每层中需要处理的单元很少，因此会导致网络性能退化。在这种情况下，我们可以说在层级方面遇到了欠拟合或容量过低的问题。降低处理能力将无法利用网络深度的优势，因此网络无法正常运行。在更深的网络结构中，处理单元分配不均的几率非常高。因此，在处理能力相同的情况下，计算单元在神经元之间的分配上，更深的网络更容易不均。当网络解决越来越深的时候，这个问题会更加凸显。换句话说，随着网络结构深度的加深，很难将神经元分配到所有层上，并且在某时所给定的计算资源将无法让网络正常运行。当更深的网络遇到缺少处理能力的问题时，就可以认为这个网络结构还不够简单，其需要必要的功能保证数据被正确的表示，这种现象我们称为**处理级枯竭**。这会导致更深的网络架构不如比较浅的网络架构。这个问题是深层网络与浅层网络性能层面比较重要的比较。因此，随着计算能力的增加，浅层网络的性能要低于更深层的网络结构，并且随着计算能力的逐步增加，这个趋势会变得越来越明显。这是因为浅层网络架构在计算量上已经饱和，会让计算力过剩，我们将这种现象称为**处理级饱和**现象，其中过剩的处理能力没有被充分的利用，所以网络结构的性能达到了饱和的状态。在计算能力充足的情况下，更深的网络架构能够更多的利用计算资源，开发出更有趣的功能，从而处理极枯竭的现象也会有进一步的改善。随着参数数量的增加，这种差异更加的明显。相应的，因为更深的网络架构在参数数量上要比更浅的网络架构更多。需要注意的是，放我们考虑增加计算能力时(适合更深的网络架构，这样所有的层都有足够的算力来执行相应的任务)，相同数量的参数会在更浅的网络结构中显得过于饱和，从而导致计算力过剩(反之亦然)。这也说明了为什么逐渐扩展和最小分配能成为关键概念，因为它们不会让你一开始就设计非常深的网络架构，其次阻止分配过多的神经元，从而有效避免处理级枯竭和处理级饱和的问题。

#### B. 同类层组

传统的网络结构设计，可以看作将若干类型的网络层(卷积、池化、归一化等)堆叠在一起。与其将设计过程看作是简单的堆叠过程，不如将网络结构看成由同样层构成的组。由几个同类型的层组，每个层组负责特定的任务(实现一个目标)。这种对称和同质的设计，不仅可以很容易地管理网络中的参数数量，同时也为每个语义提供了更好的信息池，而且还为以分组的方式，为达到更细粒度的微调和参数更新，提供了可能性。自从[22]的方法成功以来，大家都在心照不宣的在使用这项技术，并且这项技术目前已经越来越多的应用在当前主流到的网络架构中，其中就包括[2, 6-9, 54]。当然，以前这种技术只是为了让概念听起来很炫，而非对这种构建块进行充分的利用。因此，这种特征其他方面的优点并未充分利用起来。换句话说，之前大多数的用例都是这种形式，其利用了网络内部的点对点方式。尽管如此，这种方案还是对网络拓扑管理需求和参数管理大有帮助的。

#### C. 保留局部关联性



#### D. 最大化信息利用率



#### E. 最大化性能利用率



#### F. 均衡方案



####  G. 快速成型技术



#### H. 利用Dropout



#### I. 简单的自适应特征组合池



#### J. 最终阶段的规则



### 4. SimpNet



### 5. 实验结果



### 6. 总结



### 鸣谢



# 补充部分



## S1.  深入讨论



## S2.  网络结构实验

