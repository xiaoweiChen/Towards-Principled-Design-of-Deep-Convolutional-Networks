# 深度卷积网络的原理设计：一个简单的SimpNet

[原文链接](https://arxiv.org/pdf/1802.06205.pdf)

作者：

 * Seyyed Hossein Hasanpour 
* Mohammad Rouhani
* Mohsen Fayyaz
* Mohammad Sabokrou 
* Ehsan Adeli

译者：

* 陈晓伟

## 摘要

近年来，卷积神经网络(CNN)获得非常重大的成功，其代表就有VGGNet、ResNet和DenseNet等等，不过其参数达到了上亿个(几亿到几十亿之多)，这就需要更为关注计算网络所需要的计算资源，以及网络所占用的内存开销。由于这种现实的问题，限制了其在训练和优化的应用。这时，轻量级架构(比如：SqueezeNet)的提出，志在解决以上的问题。不过，因为要在计算资源和高效运行之间进行权衡，所以会产生精度不高的问题。网络低效的问题大部分源自于一种点对点的设计。本文中，在讨论过程中会为构建高效的架构提出几个关键的设计原则，并且会阐述在设计过程中对于不同方面的考虑。此外，我们会介绍一个新层——*SAF-pooling*，在加强网络的归一化能力的同时，通过选择最好的特征保证网络的简单性。根据这些原则，我们提出一个简单的架构，称为*SimpNet*。这个架构仅根据这原则设计，其在计算/存储效率和准确性之间有很好的折衷。*SimpNet*在一些知名性能测试方面的表现，要优于那些更深和更复杂的架构，比如VGGNet、ResNet和WideResidualNet等，其在参数和操作数量上要比这些网络少2到25倍。同时，我们在一些测试集上获得了很不错的结果(在模型精度和参数平衡方面)，比如CIFAR10，CIFAR100，MNIST和SVHN。*SimpNet*的实现可以在这里看到：https://github.com/Coderx7/SimpNet

## 索引词

深度学习，卷积神经网络，简单网络，分类，效率

### 1. 介绍原因

自从神经网络再次复兴以来，深度学习的方法在不同应用领域中都获得了巨大的成功，其中包括语义分割、分类、物体识别、图像标注和自然语言处理[1]。卷积神经网络对于表征学习来说是一大利器，其能在给定的数据中发现复杂的结构，并且以分层的方式对这种数据进行表示[2-4]。因此，在神经网络结构中，很少有人工设置的参数。最近，神经网络的结构发展的共同之处在于其深度和复杂度在不断的增加，这样做使给定的任务能获得更好的准确性。2015年，大规模视觉识别竞赛(ILSVRC)[5]的冠军是ImageNet，其成功之处在于使用了一个非常深的结构，具有152层[2]。当年的亚军也有非常深的结构[3]。这种让网络更深的趋势，一直沿袭至今[6-9]。

随着网络结构的加深，为了提高其辨别能力，计算资源和内存开销需要的越来越多，这也使得很难将这些网络结构用于实际应用，或者对其结构进行扩展。我们对现存的学习算法进行改进，比如：不同的初始化算法[10-13]、归一化和正则化[14-18]、非线性[11, 19-21]和数据增强[4, 14, 22–24]，当在一个性能良好的结构中使用这些方法，那么会受益匪浅。不过，这些方法中可能有些会消耗更多的计算资源，或增加内存开销[6, 15, 16]。因此，特别希望设计出具有低复杂度(少量的层数和参数)的高效网络结构，并且效果与那些更深和更复杂的网络结构一样能得到很好的结果。具有了这样高效的架构，再用刚才提到的方法进行调整，就能获得更好的效果。

设计性能良好的网络结构也有一些关键性的原则，每个原则所对应的目标有所不同，将这些原则结合起来，则能很好的增强网络结构的各个方面。遵循原则性方法，而非是一种点对点的方法。原则性方法具有允许粒度调优和自动化结构设计的优点。换言之，已经通过验证的设计原则可以用来实现一个定制化的架构，或将这些原则有策略的应用在网络架构的不同方面。另外，这种有策略的设计方式能有效的展示哪些方面对当前网络结构更具有影响力，并且可以更好的修正当前存在的问题，而不会对主要问题有所偏离，或是去针对影响力较小的方面。这种影响的展示就是可以使用相关研究，使用更加优化的算法来创建一个结构。因具有更好的识别感知，相关领域的只是可以转移到算法中去。比如，Google的AutoML就旨在自动化的设计机器学习模型[25]。

本文中，我们会介绍一些设计原则，创建一个路线图用于建立符合预期目标的网络结构。我们会对结构的不同方面进行详细的描述，例如：不同的池化操作、卷积核大小和深度等等，为架构中的底层组件提供良好的洞察力。根据我们定义的原则，我们提出了一种的新操作，名为SAF-pooling，其能提升网络的辨识能力。这层中，我们强制网络通过对最高激活值进行池化，从而学习到更加强健的特征。这里的“最高激活值”指的是，与特定类相关的强特征，并在之后会随机的对这些特征进行关闭。在这个过程中，我们模拟了由于遮挡、视点变化、光照变化等原因造成的特征不全的情况，因此网络必须通过找到新的特征检测器来适应这些情况。

考虑这些原则的原因，是因为我们要构建一个新的SimpNet架构，这个架构需要时简单并高效的。为了表明这些原则的高效性，我们会在4个主流的性能数据集(CIFAR10/100、SVHN和MNIST)上进行测试，结果显示我们的架构要优于那些更深、更复杂的网络结构，并且我们的架构减少了2到25倍的参数数量。除此之外，我们也对每个原则的进行了大量的测试，就是为了证明其必要性。简单且高效的架构(比如SimNet)会对这些原则尤为重视。这些原则为很多场景提供了高效和理想的模型架构，特别是对移动端设备(比如无人机、手机)，嵌入式系统(物联网应用)，在云边上的人工智能(AI)和深度学习(DL)应用(比如：从云中获取AI和DL应用到端点设备)。这种网络架构在未来可以使用[26,27]中提到的压缩方法进行压缩，这样对存储器和处理器的要求会有大幅度的下降。我们想要避免设计的混乱，所以采用简单和基础的方法进行设计。这样我们的注意力就不会被其他的因素所干扰，集中精力关注这些原则，确定其对性能的影响。如果不这样，工作量就会变得很大，需要面对许多来自不同方面的挑战，从而导致工作周期拉的很长，并且有可能没法去确定哪些才是真正重要的原则。从而导致远离初心，偏离主题。因此，解决方法就是从现有的文献中找寻相关的方法，并对这些方法分别进行研究。以这种方式，在模型表现的比较理想的情况下，放松对模型的约束(受一些文献中的方法的启发)，这样不需要太大的功夫就能进一步的对性能进行提升。当然，性能的提升与框架设计的水平直接相关。拙劣的设计因为其先天的缺陷，从而无法利用这些原则性的优势达到加速的效果。

本文剩余内容的组织方式：第2节，会对相关工作进行介绍；第3节，会展示我们设计的原则；第4节，展示根据我们的原则设计的SImpNet框架；第5节，展示4大数据集(CIFAR10/100、SVHN和MNIST)的实验结果，网络结构的细节和对于每个数据集预训练的不同之处；第6节，对当前工作的总结，以及对未来发展的展望。

### 2. 相关工作

这一节中，我们将分为两小节来回顾一下最新的趋势和关于网络的相关工作。然而，据我们所知，目前几乎没有对通用网络架构设计原则的研究。[8, 28, 29]在我们之前对适合于特定场景原则或策略进行过研究，虽然其中有一些原则可以应用于任何架构，但其大多数的原则还是只适用于某个特定的框架结构，所以不会对其他的场景进行测试或实验。[30]中只是列出了一些以前使用过的技术，而没有更广义的讨论起有效性和可用性。其只是一个有关使用了哪些技术的报告，并没有说明这些技术在不同情况下的有效性。从方向性上说，[28]与我们的工作最为相似。接下来，我们就简要的聊聊这些年网络结构发展的总体趋势。

#### A. 网络复杂性

从神经网络出现以来，人们都在积极地设计更为有效的网络结构[31-33]。因此出现了更加深层、更加复杂的网络结构[2-4, 6, 9, 22, 34-37]。第一群吃螃蟹的Ciresan等人[34]使用GPU训练了一个9层的*多层感知器*(MLP,  multi-layer precenteron)，然后其他的研究人员也接踵而至[2-4, 6, 9, 21, 34-38]。在其不同的研究领域中，有一些实践标准在创建和设计网络架构上起着非常重要的作用。在2012年Krizhevsky等人[22]创建了一个8层版本的LeNet5，称为AlexNet[39]。相较于LeNet5，AlexNet添加了一种新的归一化层称为局部对比归一化(译者：我看到的资料都是添加了"局部响应归一化(LRN))，和使用校正线性单元(ReLU)[21]作为非线性激活函数，用来取代原始的双曲正切方式(比如：Tanh)。当然，还有一个新的正则化层，仓位Dropout层[40]。这个网络在ILSVRC 2012[5]测试集上，在当时达到了世界领先水平。在2013年，Lin等人[41]在论文中引入了一种新的概念，其与方式已经建立好的网络发展趋势完全不同。他们提出的这个概念叫做*网中网*(NIN, network-in-network)。他们使用一个1x1的滤波器在卷积神经网络中构建微型神经网络，并使用全局池化作为结构化正则来代替全连接层，显式的将特征图转换成置信图，并在CIFAR10数据集上达到了当时领先世界的水平。在2014年时，Simonyan等人设计的VGGNet[4]引用了几种网络结构，其深度从11层增加到19层，并且表明网络的层级越深，效果越好。他们也使用了3x3卷积层，并说明使用更小尺寸的滤波器对非线性有更好的拟合性，同时能获得更好的精度。同年中，受NIN结构的启发产生了一种新的网络结构GoogleNet[3]，56个卷积层构成了一个具有22个模块化层级的网络。模块是由几个1x1，3x3和5x5，这种结构称为inception模块。与以往的网络结构相比，这种结构极大的减少了模型参数的数量，并且在ILSVRC测试中达到了当时的最高水准。在比较新的Inception版本中，5x5的卷积层被替换成两个连续的3x3卷积层。在网络中使用批量归一化[16]可以减少网络内部协变量的偏移；这种方法在训练深层网络结构是十分重要的，并在ImageNet的挑战中刷新了当时的最好成绩。

在2015年，He等人[11]使用VGGNet19[4]再次刷新了ILSVRC的成绩，其使用参数化ReLU层(PReLU)来替代ReLU，从而改进了模型的拟合性。其还引入了一种新的初始化方法，从而能用来增强新非线性激活层的性能。网络层间的连通性在当时是一个全新的概念，其目的就是增强网络中的梯度传递和信息流。相关原理介绍分别在[2]和[37]中。在[2]中，会对残差块进行介绍，说明那些曾适合进行残差映射。在前人的研究基础上，他们成功的训练出从152层到1000层的极深网络结构，并再次刷新了ILSVRC的成绩。同样，[37]中提出了一个递归神经网络(即使用自适应门单元调节网络中的信息流)，其灵感来自于长短期记忆(LSTM)。这种递归神经网络也很成功的训练出从100层到1000层的网络。

在2016年，Szegedy等人[7]将残差链接与他们的Inception-V3结构相结合，具有更好的效果。他们也给出了相关经验证明，使用残差链接对Inception进行训练，可以显著的加速训练速度，并且说明残差版本的Inception网络的性能与原始版本的Inception网络差不多。随着网络这样的发展， ILSVRC 2012分类任务[5]在单帧识别的性能上，有了巨大的提升。 Zagoria等人[9]在被称为宽残差网络(WRN)的的残差网络[2]上进行了详细的实验，与以往瘦深的网络不同，其扩展了网络的宽度，而非深度(即降低深度)。这种新的网络结构，并不会有减少特征重用的问题，以及训练时间慢的问题。Huang等人[6]引入了一种新的层间链接方式，称为DenseBlock，其中每层以前馈的方式直接连接到其他的层。这种连同模式减轻了梯度消失的问题，并增强了特征的传播。虽然跨层链接有所增加，但是其古丽使用特征复用，从而极大的减少了模型参数的数量。这种模型的泛化能力出奇的好，并在好几个测试集上刷新了性能的成绩，比如CIFAR 10/100和SVHN。

#### B. 轻量级架构



### 3. 设计原则



### 4. SimpNet



### 5. 实验结果



### 6. 总结



### 鸣谢



# 补充部分



## S1.  深入讨论



## S2.  网络结构实验

